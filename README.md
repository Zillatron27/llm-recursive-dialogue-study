# An Observational Study of Recursive Self-Referential Behavior in Large Language Model Conversations

üìÑ **[Download the full paper (PDF)](paper.pdf)**  
*Current version: v0.3 ‚Äî revised to clarify operational definitions, replication details, results classification and additional clarity pass.*

---

## Overview

Extended interactions with large language models (LLMs) are sometimes reported to produce unusual conversational behaviors, including recursive self-reference, meta-level analysis of the interaction itself, and shifts in tone or framing over time. These reports are often dismissed as anthropomorphic interpretation or attributed solely to interaction length, without systematic comparison or control.

This repository accompanies an observational study that documents and compares a small number of extended LLM conversations conducted under differing conditions. The focus of the study is on **interaction-level conversational dynamics**, rather than on claims about internal model states, mechanisms, or subjective experience.

The goal is careful characterization rather than interpretation or theory-building.

---

## What this repository contains

- A preprint describing the study design, observational criteria, results, and limitations.
- Verbatim transcripts of all conversational sessions analyzed in the paper, including:
  - An unprimed baseline session.
  - Follow-up sessions conducted with different model variants and memory contexts.
  - A long-horizon control session spanning ~30,000 tokens.
  - A partial replication attempt conducted on a separate account.
- An executive summary and methodological clarifications included in the manuscript to support non-specialist readers.

All transcripts are provided without modification and correspond directly to sessions described in the paper.

---

## What this study *is*

- An observational, descriptive analysis of conversational behavior in extended LLM interactions.
- Focused on identifying when specific interaction-level patterns appear, fail to appear, or diverge.
- Explicitly inclusive of null results and failed replications.
- Constrained to externally observable behavior in text-based dialogue.

---

## What this study *is not*

- ‚ùå A claim about consciousness, sentience, or subjective experience.
- ‚ùå A mechanistic explanation of model behavior.
- ‚ùå Proof of emergent agency or intent.
- ‚ùå A generalization across all models or usage contexts.

---

## Data

All chat transcripts referenced in the accompanying preprint are provided verbatim in the `transcripts/` directory. File names correspond to the session labels used in the paper.

No transcripts have been edited for content or tone.

---

## Status

This repository represents an initial public release of an evolving observational study.  
The current manuscript reflects a revised version incorporating clarified operational definitions, corrected replication details, and strengthened control comparisons.

---

## License

This repository is released under the  
[Creative Commons Attribution 4.0 International (CC BY 4.0)](LICENSE) license.

---

## How to cite

If you reference this work, please cite:

*An Observational Study of Recursive Self-Referential Behavior in Large Language Model Conversations*  
Available at: https://github.com/Zillatron27/llm-recursive-dialogue-study
